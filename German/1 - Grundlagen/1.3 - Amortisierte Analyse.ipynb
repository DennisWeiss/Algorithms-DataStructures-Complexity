{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amortisierte Analyse\n",
    "\n",
    "Nicht immer ist die Angabe der worst-case Komplexität hilfreich. Was ist nämlich, wenn der worst-case nur sehr selten eintritt, aber in den meisten Fällen eine wesentlich bessere Laufzeit erzielt? Dies ist z.B. beim Hinzufügen von Elementen an eine Array-List der Fall. In den meisten Fällen geht dies problemlos mit einem Aufwand von $\\mathcal{O}(1)$, doch ist das Array nicht lang genug, so müssen alle Werte in eine neues Array kopiert werden, was zu einem Aufwand von $\\mathcal{O}(n)$ führt. Dadurch liegt die worst-case Komplexität bei $\\mathcal{O}(n)$, was zunächst ziemlich schlecht zu sein scheint, da andere Datenstrukturen dies (selbst im worst-case) in $\\mathcal{O}(1)$ können. Um hier eine genaue Aussage treffen zu können, bedarf es der amortisierten Analyse.\n",
    "\n",
    "Die amoritsierte Analyse (amortized analysis) berechnet die mittleren Kosten über einer Folge von Operationen, in der viele dieser Operationen billig sind und nur wenige teuer in Bezug auf deren Beitrag zur Gesamtzeit.\n",
    "\n",
    "## Aggregat-Analyse\n",
    "\n",
    "Bei der Aggregat-Analyse gilt:\n",
    "\n",
    "$$T(n)_{amort} = \\frac{\\text{Summe der Kosten aller Operationen}}{\\text{Anzahl der Operationen}}$$\n",
    "\n",
    "Die Kosten der Operationen $1, 2, ..., n$ wird durch die Anzahl $n$ der Operationen geteilt.\n",
    "\n",
    "### Array-List\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
